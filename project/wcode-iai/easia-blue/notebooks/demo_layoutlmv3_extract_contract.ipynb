{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 LayoutLMv3 Demo: Extract Contract Key-Value from PDF (Text or Scan)\n",
        "\n",
        "This notebook demonstrates how to use `LayoutLMv3` from HuggingFace Transformers to extract structured information from contract PDFs.\n",
        "\n",
        "You can use this for scanned documents (with OCR) or text-based PDFs.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2705 Install required packages\n",
        "!pip install transformers datasets pytesseract pdf2image torch torchvision --quiet\n",
        "!apt install poppler-utils tesseract-ocr -y  # For pdf2image and pytesseract on Colab (Linux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2705 Imports\n",
        "import pytesseract\n",
        "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "from pdf2image import convert_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2705 Load pretrained model and processor\n",
        "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=True)\n",
        "model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2705 Convert PDF to image\n",
        "images = convert_from_path(\"../store/input/your_contract_file.pdf\", dpi=300)\n",
        "image = images[0]  # use first page\n",
        "image.save(\"page.jpg\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2705 Prepare input for LayoutLMv3\n",
        "encoding = processor(image, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoding)\n",
        "\n",
        "predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
        "tokens = processor.tokenizer.convert_ids_to_tokens(encoding['input_ids'].squeeze())\n",
        "\n",
        "for token, pred in zip(tokens, predictions):\n",
        "    if pred != 0:\n",
        "        print(f\"Token: {token}, Label ID: {pred}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}